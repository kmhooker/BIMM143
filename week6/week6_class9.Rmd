---
title: "Unsupervised Learning Analysis of Human Breast Cancer Cells"
author: "Kiley Hooker (A15441609)"
date: "2/13/2022"
output:
  pdf_document: default
  html_document: default
---
## Preparing the data
```{r}
fna.data <- "WisconsinCancer.csv"
wisc.df <- read.csv(fna.data, row.names=1)
#wisc.df
wisc.data <- wisc.df[,-1]
```

Create a new vector called 'diagnosis' that contains the data from the diagnosis column of the orignal data set and store as a factor
```{r}
diagnosis <- wisc.df$diagnosis
```

## Exploratory data analysis

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```
212 are malignant.

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
length(grep(pattern="_mean", colnames(wisc.data)))
```

# Principle Component Analysis

## Performing PCA

Check column means and standard deviations
```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

Perform PCA on wisc.data and look at summary of results
```{r}
wisc.pr <- prcomp(x=wisc.data, scale=TRUE)
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

3 PCs

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

7 PCs

## Interpreting PCA results

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

The graph is very chaotic and messy making it very difficult to read. It's hard to understand because of the rownames being used as the data point characters.

Scatter plot observations by components 1 and 2
```{r}
plot(wisc.pr$x[,1:2], col=as.factor(diagnosis),
     xlab="PC1", ylab="PC2")
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,c(1,3)], col=as.factor(diagnosis),
     xlab="PC1", ylab="PC3")
```

It's much easier to visualize the data in these plots as it isn't so crowded and you can actually see the separation between the red(malignant) and black(benign) data points. PC1vsPC2 and PC1vsPC3 only differ in their y-axis.


```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

## Variance explained

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
## ggplot based graph
# install.packages("factoextra")
# library(factoextra)
# fviz_eig(wisc.pr, addlabels = TRUE)
```

## Communicating PCA results
> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

 -0.26085376
```{r}
wisc.pr$rotation[,1]
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

5 PCs


# Hierarchial clustering

```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)

# Calculate the Euclidean distances between all pairs of observations
data.dist <- dist(data.scaled)

# Create a hierarchical clustering model using complete linkage
wisc.hclust <- hclust(data.dist, method = "complete")
```

## Results of hierarchial clustering
> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

At height 19, the model has 4 clusters.
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

## Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)
```

> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

After experimenting with clusters between 2 and 10, cutting by into clusters of 2 presents the most simplified results. 

## Using different methods

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

"ward.D2" gives my favorite results because the data is easiest to read due to the limited variance within clusters.


# OPTIONAL: K-means clustering
## K-means clustering and comparing results

```{r}
wisc.km <- kmeans(scale(wisc.data), centers= 2, nstart= 20)
table(wisc.km$cluster, diagnosis)
```

> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

Clusters 1, 2, and 4 from hclust is equivalent to cluster 1 from kmeans while cluster 3 is the kmeans cluster 2.

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```


# Combining methods
## Clustering on PCA results
```{r}
data.dist <- dist(wisc.pr$x[,1:7])
wisc.pr.hclust <- hclust(data.dist, method = "ward.D2")
plot(wisc.pr.hclust)
```
```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=as.factor(diagnosis))
```

```{r}
g <- as.factor(grps)
levels(g)
```
```{r}
g <- relevel(g,2)
levels(g)
```
```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```
```{r}
# library(rgl)
# plot3d(wisc.pr$x[,1:3], xlab="PC 1", ylab="PC 2", zlab="PC 3", cex=1.5, size=1, type="s", col=grps)
```

```{r}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
data.dist <- dist(wisc.pr$x[,1:7])
wisc.pr.hclust <- hclust(data.dist, method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

> Q15. How well does the newly created model with two clusters separate out the two diagnoses?

The newly created model separates the two diagnoses pretty well. There is a clear distinction between the malignant and benign tumors shown by the color. However the slight overlap between the black and red makes it a little unclear.
```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

K-means and hclust do alright with separating the diagnoses as you can tell there are 2 main clusters in each, but the PCA model does it best.
```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

# Sensitivity/Specificity
> Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

Both kmeans and hclust have the best specificity, while kmeans has the best sensitivity.

Specificity = TP/(TP+FN)
```{r}
# for hclust
343/(343+12+2)
# for kmeans
343/(343+14)
```
Sensitivity = TN/(TN+FN)
```{r}
# for hclust
165/(165+5+40+2)
# for kmeans
175/(175+37)
```


# Prediction
```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

We should prioritize patient 2 for follow up due to the location in the red cluster, which signifies malignant cells. 

```{r}
sessionInfo()
```

